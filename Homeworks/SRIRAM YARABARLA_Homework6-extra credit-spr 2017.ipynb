{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sentiment_stream.py\n",
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = \"925147187184635904-DdhQn8xZKPVFwHb68mYUHuhzpi8Anzu\"\n",
    "access_token_secret = \"8I4Lg5fz5sNPawKccZXpjQCu4azXEa3UbFNjvyAe2xTWY\"\n",
    "consumer_key = \"KT6cdia0sVGwrmyqX0m4xXbac\"\n",
    "consumer_secret = \"ovXZkyp7k4jih8LalVRVHfLCxZTn9TztolHmMCvbBtOyDLLVYw\"\n",
    "\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print data\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'football', 'eagles', 'cowboys', 'packers' , 'giants'.\n",
    "    stream.filter(track=['selfdrive', 'automatic', 'tesla', 'autonomous'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cmd\n",
    "python sentiment_stream.py>sentiment.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "tweets_data_path = 'sentiment.txt'\n",
    "\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print len(tweets_data)\n",
    "\n",
    "tweets = pd.DataFrame()\n",
    "tweets['text'] = map(lambda tweet: tweet['text'], tweets_data)\n",
    "tweets['lang'] = map(lambda tweet: tweet['lang'], tweets_data)\n",
    "tweets['time_zone'] = map(lambda tweet: tweet['user']['time_zone'], tweets_data)\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, nltk, re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_data(tweets_data_path):\n",
    "    tweets_data = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tweets_file = open(tweets_data_path, \"r\")\n",
    "    for line in tweets_file:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            if tweet[\"lang\"]=='en':\n",
    "                tweet_text = tweet[\"text\"]\n",
    "                #print tweet_text\n",
    "                filter_url = re.sub(r\"http\\S+\", \"\", tweet_text)\n",
    "                #print filter_url\n",
    "                filter_names = re.sub(r\"RT+.*\\:\", \"\", filter_url)\n",
    "                #print filter_names\n",
    "                filter_splchar = re.sub(\"[^A-Z a-z]+\", \"\", filter_names)\n",
    "                #print filter_splchar\n",
    "                filter_stop = [word for word in (filter_splchar.lower()).split() if word not in stop_words]\n",
    "                result = ' '.join(filter_stop)\n",
    "                #print result\n",
    "                tweets_data.extend(result.split())       \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'sentiment.txt'\n",
    "tweet_data = clean_data(data_path)\n",
    "words = set(tweet_data)\n",
    "file = open('sentiment_words.txt', 'w')\n",
    "for item in list(words):\n",
    "    file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_train_words=pd.DataFrame(tweet_data)\n",
    "pd.DataFrame(top_train_words[0].value_counts()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybloom import BloomFilter\n",
    "import os\n",
    "import re\n",
    "\n",
    "post_dir = 'C:\\\\Users\\\\srira\\\\Desktop\\\\positive\\\\'\n",
    "# Read all my posts.\n",
    "posts = {post_name: open(post_dir + post_name).read() for post_name in os.listdir(post_dir)}\n",
    "# Create a dictionary of {\"post name\": \"lowercase word set\"}.\n",
    "split_posts = {name: set(re.split(\"\\W+\", contents)) for name, contents in posts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybloom import BloomFilter\n",
    "import os\n",
    "import re\n",
    "\n",
    "post_dir = 'C:\\\\Users\\\\srira\\\\Desktop\\\\negative\\\\'\n",
    "# Read all my posts.\n",
    "posts = {post_name: open(post_dir + post_name).read() for post_name in os.listdir(post_dir)}\n",
    "# Create a dictionary of {\"post name\": \"lowercase word set\"}.\n",
    "split_posts = {name: set(re.split(\"\\W+\", contents)) for name, contents in posts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filters = {}\n",
    "for name, words in split_posts.items():\n",
    "    print name, words\n",
    "    filters[name] = BloomFilter(capacity=len(words), error_rate=0.1)\n",
    "    for word in words:\n",
    "        filters[name].add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(search_string):\n",
    "    search_terms = re.split(\"\\W+\", search_string)\n",
    "    return [name for name, filter in filters.items() if all(term in filter for term in search_terms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "matched_words = []\n",
    "path = 'C:\\\\Users\\\\srira\\\\Desktop\\\\sentiment\\\\'\n",
    "post = {post_name: open(path + post_name).read() for post_name in os.listdir(path)}\n",
    "# Create a dictionary of {\"post name\": \"lowercase word set\"}.\n",
    "split_pst = {name: set(re.split(\"\\W+\", contents.lower())) for name, contents in post.items()}\n",
    "#split_posts\n",
    "for name, words in split_pst.items():\n",
    "    for word in words:\n",
    "        if search(word)==['positive.txt']:\n",
    "            matchedpositive_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "matched_words = []\n",
    "path = 'C:\\\\Users\\\\srira\\\\Desktop\\\\sentiment\\\\'\n",
    "post = {post_name: open(path + post_name).read() for post_name in os.listdir(path)}\n",
    "# Create a dictionary of {\"post name\": \"lowercase word set\"}.\n",
    "split_pst = {name: set(re.split(\"\\W+\", contents.lower())) for name, contents in post.items()}\n",
    "#split_posts\n",
    "for name, words in split_pst.items():\n",
    "    for word in words:\n",
    "        if search(word)==['negative.txt']:\n",
    "            matchednegative_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_match_words = [x for x in sentiment_words if x in matchedpositive_words]\n",
    "sort_match_positive = sorted(positive_match_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_match_words = [x for x in sentiment_words if x in matchednegative_words]\n",
    "sort_match_negative = sorted(neative_match_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(sort_match_positive)\n",
    "matchpos=a[0].value_counts().head(10)\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = pd.DataFrame(sort_match_negative)\n",
    "matchneg=b[0].value_counts().head(10)\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = matchpos.plot(x=0, y=1, kind=\"bar\", figsize=(10, 10), fontsize=15)\n",
    "ax.set_title('Matched positive Words Frequency', fontsize=15)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=10)\n",
    "ax.set_xlabel(\"Words\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = matchneg.plot(x=0, y=1, kind=\"bar\", figsize=(10, 10), fontsize=15)\n",
    "ax.set_title('Matched negative Words Frequency', fontsize=15)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=10)\n",
    "ax.set_xlabel(\"Words\", fontsize=10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
