{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Homework 5\n",
    "Copy this notebook. Rename it as: YOURNAME-HW5-streams with your name replacing YOURNAME.\n",
    "Upload your completed jupyter notebook to elearning site as your homework submission. You can put this notebook on your github.\n",
    "\n",
    "5.1 Register for a stream of Twitter data\n",
    "5.2 Create a bloom filter classifying two days worth of twitters ( after removing stop words and urls )\n",
    "5.3 For another days worth of twitter data find the previous twitters that match in the bloom filter (This means get two days of data in one file or directory , use that data for training the bloom filter, capture a different days data in a different file ( or do it in real time)and capture the match output then running the new twitter data through the filter.\n",
    "5.4 Plot a historgram of matches for each twitter in 5.3\n",
    "For the 4-5 grade.- Submit in a separate notebook - YourNAME-Homework5-Supplement\n",
    "Use a different machine learning training algorithm\n",
    "Make a continous feed where you take two days of data and match the incoming stream ( do this for 5 days windowing the filter data)\n",
    "Find new trends in the twitter feed (daily or hourly)\n",
    "Or some other streaming exploration of your choosing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturing Dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load twitter_stream.py\n",
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = \"925147187184635904-DdhQn8xZKPVFwHb68mYUHuhzpi8Anzu\"\n",
    "access_token_secret = \"8I4Lg5fz5sNPawKccZXpjQCu4azXEa3UbFNjvyAe2xTWY\"\n",
    "consumer_key = \"KT6cdia0sVGwrmyqX0m4xXbac\"\n",
    "consumer_secret = \"ovXZkyp7k4jih8LalVRVHfLCxZTn9TztolHmMCvbBtOyDLLVYw\"\n",
    "\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print data\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'football', 'eagles', 'cowboys', 'packers' , 'giants'.\n",
    "    stream.filter(track=['football', 'eagles', 'cowboys', 'packers' , 'giants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cmd\n",
    "python twitter_stream.py > training_twiter.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturing dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load twitter_stream1.py\n",
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = \"925147187184635904-DdhQn8xZKPVFwHb68mYUHuhzpi8Anzu\"\n",
    "access_token_secret = \"8I4Lg5fz5sNPawKccZXpjQCu4azXEa3UbFNjvyAe2xTWY\"\n",
    "consumer_key = \"KT6cdia0sVGwrmyqX0m4xXbac\"\n",
    "consumer_secret = \"ovXZkyp7k4jih8LalVRVHfLCxZTn9TztolHmMCvbBtOyDLLVYw\"\n",
    "\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print data\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'football', 'eagles', 'cowboys', 'packers' , 'giants'.\n",
    "    stream.filter(track=['eagles','Pederson','smith'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cmd\n",
    "python twitter_stream1.py > checking_twitter.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\srira\\\\Desktop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "tweets_data_path = 'training_twitter.txt'\n",
    "\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print len(tweets_data)\n",
    "\n",
    "tweets = pd.DataFrame()\n",
    "tweets['text'] = map(lambda tweet: tweet['text'], tweets_data)\n",
    "tweets['lang'] = map(lambda tweet: tweet['lang'], tweets_data)\n",
    "#tweets['time_zone'] = map(lambda tweet: tweet['user']['time_zone'], tweets_data)\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I had tried upto a week to write up pandas and clean the text but I cannot do it,so I took help of my friends(Sravani) idea and written this code\n",
    "#I promise I have learned every part of it.\n",
    "#including my code as well in the next code section.\n",
    "import json, nltk, re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_data(tweets_data_path):\n",
    "    tweets_data = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tweets_file = open(tweets_data_path, \"r\")\n",
    "    for line in tweets_file:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            if tweet[\"lang\"]=='en':\n",
    "                tweet_text = tweet[\"text\"]\n",
    "                #print tweet_text\n",
    "                filter_url = re.sub(r\"http\\S+\", \"\", tweet_text)\n",
    "                #print filter_url\n",
    "                filter_names = re.sub(r\"RT+.*\\:\", \"\", filter_url)\n",
    "                #print filter_names\n",
    "                filter_splchar = re.sub(\"[^A-Z a-z]+\", \"\", filter_names)\n",
    "                #print filter_splchar\n",
    "                filter_stop = [word for word in (filter_splchar.lower()).split() if word not in stop_words]\n",
    "                result = ' '.join(filter_stop)\n",
    "                #print result\n",
    "                tweets_data.extend(result.split())       \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return tweets_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#I had tried upto a week to write up pandas and clean the text but I cannot do it,so I took help of my friends(Sravani) idea and written this code\n",
    "#I promise I have learned every part of it.\n",
    "#including my code as well in the next code section.\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "tweets_data_path = 'training_twitter.txt'\n",
    "\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print len(tweets_data)\n",
    "\n",
    "tweets = pd.DataFrame()\n",
    "tweets['text'] = map(lambda tweet: tweet['text'], tweets_data)\n",
    "tweets['lang'] = map(lambda tweet: tweet['lang'], tweets_data)\n",
    "#tweets['time_zone'] = map(lambda tweet: tweet['user']['time_zone'], tweets_data)\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "tweets\n",
    "\n",
    "tweets.loc[tweets['lang'] == 'en']   #remove other launguages\n",
    "tweets.text.str.encode('utf-8').str.decode('ascii', 'ignore')  #remove special characters\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "#import sys  \n",
    "\n",
    "\n",
    "#remove stop words in tweets\n",
    "tweets['text_without_stopwords'] = tweets['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "tweets['text_without_stopwords'] = tweets['text_without_stopwords'].str.replace('http\\S+|www.\\S+', '', case=False) #remove URL'S\n",
    "tweets['text_without_stopwords'] = tweets['text_without_stopwords'].str.replace('RT', '', case=False)\n",
    "tweets['text_without_stopwords'] = tweets['text_without_stopwords'].str.replace('@', '', case=False)\n",
    "tweets['text_without_stopwords'] = tweets['text_without_stopwords'].str.replace('#', '', case=False)\n",
    "tweets['text_without_stopwords'] = tweets['text_without_stopwords'].str.replace(':', '', case=False)\n",
    "tweets.text_without_stopwords.str.encode('utf-8').str.decode('ascii', 'ignore')  #remove special characters\n",
    "\n",
    "list(tweets['text_without_stopwords'].str.split(' ', expand=True).stack().unique())  #To get all words in the \"text_without_stopwords\" column:\n",
    "tweets['text_without_stopwords'].str.split(' ', expand=True).stack().value_counts()  #To get counts of \"review\" column:\n",
    "\n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding('utf8')\n",
    "\n",
    "np.savetxt('trainingdata.txt', tweets, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path = 'training_twitter.txt'\n",
    "train_tweet_data = clean_data(train_data_path)\n",
    "train_words = set(train_tweet_data)\n",
    "train_file = open('train_words.txt', 'w')\n",
    "for item in list(train_words):\n",
    "    train_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_train_words=pd.DataFrame(train_tweet_data)\n",
    "pd.DataFrame(top_train_words[0].value_counts()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_data_path = 'checking_twitter.txt'\n",
    "check_tweet_data = clean_data(check_data_path)\n",
    "check_words = set(check_tweet_data)\n",
    "check_file = open('check_words.txt', 'w')\n",
    "for item in list(train_words):\n",
    "    check_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_check_words=pd.DataFrame(check_tweet_data)\n",
    "pd.DataFrame(top_check_words[0].value_counts()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybloom import BloomFilter\n",
    "import os\n",
    "import re\n",
    "\n",
    "post_dir = 'C:\\\\Users\\\\srira\\\\Desktop\\\\training\\\\'\n",
    "# Read all my posts.\n",
    "posts = {post_name: open(post_dir + post_name).read() for post_name in os.listdir(post_dir)}\n",
    "# Create a dictionary of {\"post name\": \"lowercase word set\"}.\n",
    "split_posts = {name: set(re.split(\"\\W+\", contents)) for name, contents in posts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {}\n",
    "for name, words in split_posts.items():\n",
    "    print name, words\n",
    "    filters[name] = BloomFilter(capacity=len(words), error_rate=0.1)\n",
    "    for word in words:\n",
    "        filters[name].add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(search_string):\n",
    "    search_terms = re.split(\"\\W+\", search_string)\n",
    "    return [name for name, filter in filters.items() if all(term in filter for term in search_terms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "matched_words = []\n",
    "path = 'C:\\\\Users\\\\srira\\\\Desktop\\\\checking\\\\'\n",
    "post = {post_name: open(path + post_name).read() for post_name in os.listdir(path)}\n",
    "# Create a dictionary of {\"post name\": \"lowercase word set\"}.\n",
    "split_pst = {name: set(re.split(\"\\W+\", contents.lower())) for name, contents in post.items()}\n",
    "#split_posts\n",
    "for name, words in split_pst.items():\n",
    "    for word in words:\n",
    "        if search(word)==['train_words.txt']:\n",
    "            matched_words.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match_words = [x for x in check_words if x in matched_words]\n",
    "sort_match = sorted(match_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(sort_match)\n",
    "match=a[0].value_counts().head(10)\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = match.plot(x=0, y=1, kind=\"bar\", figsize=(10, 10), fontsize=15)\n",
    "ax.set_title('Matched Words Frequency', fontsize=15)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=10)\n",
    "ax.set_xlabel(\"Words\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
